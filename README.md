# ê°€ë‚˜ë‹¤ë¼ë§ˆ-GAN Model í•œê¸€ ì† ê¸€ì”¨ ì´ë¯¸ì§€ ìƒì„±
# Korean GAN : Generating Handwritten Korean Using Generative Adversarial Network (with tf.keras)


![Dataset](https://img.shields.io/badge/Dataset-KOREAN-yellow.svg)
![Language](https://img.shields.io/badge/Language-Python-blue.svg)
[![DeepLearning](https://img.shields.io/badge/DeepLearning-tf.keras-red.svg)](https://keras.io)
![Model](https://img.shields.io/badge/Model-GAN-green.svg)

>  [iOS ëª¨ë°”ì¼ì—ì„œ í•œê¸€ ì†ê¸€ì”¨ ì¸ì‹í•˜ê¸°(with Keras)](https://github.com/MijeongJeon/KoreanClassification_Keras_Coreml)ì™€ ì´ì–´ì§€ëŠ” í”„ë¡œì íŠ¸ë¡œ ë” ë§ì€ í•œê¸€ ë°ì´í„°ì…‹ êµ¬ì¶•ì„ ìœ„í•œ GAN ëª¨ë¸ í™œìš©ì— ê´€í•´ ì´ì•¼ê¸°í•©ë‹ˆë‹¤

**GAN ëª¨ë¸ë¡œ ìƒì„±ëœ í•œê¸€ ì† ê¸€ì”¨ ğŸŒ±**

![](./media/ganadalama.gif)

---

## Train Dataset
![](./media/image1.png)

```
* Image Size: 32 X 32(pixel)
* Class Number : 16(ê°€, ë‚˜, ë‹¤, ë¼, ë§ˆ, ë°”, ì•„, ì‚¬, ì, ì°¨, íƒ€, íŒŒ, í•˜, ì „, ë¯¸, ì •) 
* Total Images : 9,962
```

- ì§ì ‘ ì‘ì„±í•œ *ë¯¸ì • í•„ê¸°ì²´*ë¥¼ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.
- ê°œì¸ìš© ë˜ëŠ” ìƒì—…ìš© ì‚¬ìš©ì´ ê°€ëŠ¥í•œ 46ê°œì˜ í•œê¸€ ì†ê¸€ì”¨ í°íŠ¸(*.ttf)ì„ ì´ìš©í•´ ë°ì´í„°ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.
- TTF íŒŒì¼ì„ ì´ìš©í•´ ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” ë°©ë²•ì€ [IBM Developer ì‚¬ì´íŠ¸ì— Paul Van Eckì˜ í¬ìŠ¤íŒ…](https://developer.ibm.com/kr/journey/create-a-mobile-handwritten-hangul-translation-app)ì„ ì°¸ê³ í–ˆìŠµë‹ˆë‹¤.

### Image Augmentation
- [scikit-image](https://scikit-image.org)ë¥¼ í™œìš©í•´ ìƒí•˜ì¢Œìš° ì´ë™, íšŒì „ëœ ì´ë¯¸ì§€ ì¶”ê°€ë¡œ ë°ì´í„°ë¥¼ ì¦í­í–ˆìŠµë‹ˆë‹¤

	```python
	from skimage.transform import rotate, AffineTransform, warp
		
	left_image = rotate(img, angle=15, cval=1)  
	right_image1 = rotate(img, angle=-15, cval=1) 
		
	hori_transform = AffineTransform(translation=(27,0))  
	warp_r_image = warp(img, hori_transform, mode="wrap")
		
	verti_transform = AffineTransform(translation=(0,27))  
	warp_l_image = warp(img, verti_transform, mode="wrap")
	```

![](./media/image4.png)
![](./media/image3.png)


> (English)  
- I created image data using 46 Korean handwriting fonts (* .ttf) that are available for personal or commercial use.  
- And I've added my own Mijeong Cursive.  
- For information on how to generate image data using TTF files, see [the article posted by Paul Van Eck on the IBM Developer site](https://developer.ibm.com/kr/journey/create-a-mobile-handwritten-hangul-translation-app/)  
- Used [scikit-image](https://scikit-image.org) to augment the image data by adding rotation and transformation.

## GAN with tf.keras ğŸ¤–
```
* Keras Version: 2.4.0  
* Backend: TensorFlow 2.3.0    
```
[GAN model training code in Jupyter Notebook](./KoreanGAN_GeneratingHandwrittenKoreanUsingGenerativeAdversarialNetwork.ipynb)

### GAN minMax Theory
![](http://dl.dropbox.com/s/0ro2ny0enzmpx8n/gan_min_max.png)

`G: Generator, D: Discriminaror, x: real image, z: noise`

* Generator(ìƒì„±ì)ì˜ ì—­í• 
	*  ì§„ì§œì™€ ìœ ì‚¬í•œ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•´ íŒë³„ìë¥¼ ì†ì¸ë‹¤ 
* Discriminator(íŒë³„ì)ì˜ ì—­í• 
	* 1. ì§„ì§œ ì´ë¯¸ì§€ë¥¼ ì§„ì§œë¼ê³  íŒë³„í•´ì•¼í•œë‹¤ (output ~ 1)
	* 2. ìƒì„±ìê°€ ë§Œë“  ê°€ì§œ ì´ë¯¸ì§€ë¥¼ ë³´ê³  ê°€ì§œë¼ê³  íŒë³„í•´ì•¼í•œë‹¤ (output ~ 0)

### Generator(ìƒì„±ì ëª¨ë¸)
* inputìœ¼ë¡œ 100ê°œì˜ noiseë¥¼ ë°›ì•„ (32*32*1) imageë¥¼ ìƒì„±í•œë‹¤
	
```python
def build_generator_model():
    model = tf.keras.Sequential() # Keras ëª¨ë¸ ìƒì„±
	
    model.add(layers.Dense(1024, input_dim=100, use_bias=False))
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())
    
    model.add(layers.Dense(8*8*128, use_bias=False))
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())
    
    # Resahpe (8*8)
    model.add(layers.Reshape((8, 8, 128)))  
    
    model.add(layers.Conv2DTranspose(128, (5, 5), 
                                    strides=(1, 1), padding='same', use_bias=False))
    model.add(layers.BatchNormalization()) 
    model.add(layers.LeakyReLU())
    
    # (8*8) -> (16*16)
    model.add(layers.Conv2DTranspose(64, (5, 5), 
                                    strides=(2, 2), padding='same', use_bias=False))
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())
    
    # (16*16) -> (32*32)
    model.add(layers.Conv2DTranspose(1, (5, 5), 
                                    strides=(2, 2), padding='same', activation='tanh'))
    assert model.output_shape == (None, 32, 32, 1)
	
    return model
```	

### Discriminator(íŒë³„ì ëª¨ë¸)
* inputìœ¼ë¡œ (32*32*1) imageë¥¼ ë°›ì•„ 0~1 ì‚¬ì´ì˜ output ê°’ì„ ì¤€ë‹¤(ì´ë¯¸ì§€ê°€ ì§„ì§œë¼ê³  íŒë³„ë˜ë©´ 1, ê°€ì§œë¼ê³  íŒë³„ë˜ë©´ 0)

```python
def build_discriminator_model():

    model = tf.keras.Sequential()
    
    model.add(layers.Conv2D(64, (5, 5), strides=2, padding='same', 
                       input_shape=[32, 32, 1])) # input image size
    model.add(layers.LeakyReLU(0.2))
    model.add(layers.Dropout(0.3))

    model.add(layers.Conv2D(128, (5, 5), strides=2, padding='same'))
    model.add(layers.LeakyReLU(0.2))
    
    model.add(layers.Flatten())
    
    model.add(layers.Dense(256))
    model.add(layers.LeakyReLU(0.2))
    model.add(layers.Dropout(0.3))

    model.add(layers.Dense(1))
    
    return model
```

## Author ğŸ¤“ 
![](./media/image5.png)  
Mijeong Jeon   
[ninevincentg@gmail.com](mailto:ninevincentg@gmail.com)


## í”„ë¡œì íŠ¸ ì„¤ëª… ì˜ìƒìœ¼ë¡œ ë³´ê¸° ğŸ•Š
[ğŸ’» Youtube ì˜ìƒ ë³´ê¸°](https://youtu.be/z-LIpUX-lpc)
[![](./media/image6.png)](https://youtu.be/z-LIpUX-lpc)
